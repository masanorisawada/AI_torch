{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorchで画像分類\n",
    "## MLPによるのCIFAR10分類\n",
    "## ミニバッチ学習\n",
    "## （ローカルPC上でCPU or GPU） or AWSクラウドGPU の利用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch#tensorを作成・操作するための機能、GPUの仕様も設定\n",
    "import torch.nn as nn#nnを構成する部品を提供\n",
    "import torch.optim as optim#SGD,adamなどの最適化関数や学習率を設定\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習に使用するデバイス： cuda\n"
     ]
    }
   ],
   "source": [
    "# もしGPUが利用可能なら cuda が入る\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print('学習に使用するデバイス：',device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前処理の定義 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\", \n",
    "    train=True, download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\", \n",
    "    train=False, download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "img,label = train_dataset[39]\n",
    "print(img.size())\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ミニバッチのサイズを決める\n",
    "### バッチ学習：60000 枚を一度に学習する\n",
    "### ミニバッチ学習：例えば 60000 枚を 100 枚ずつに分けて学習する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: ./data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100個ごとに画像と教師データをセットにして train_dataloaderに入れる\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=num_batches, \n",
    "    shuffle=True\n",
    ")\n",
    "train_dataloader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ./data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100個ごとに画像と教師データをセットにして train_dataloaderに入れる\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=num_batches, \n",
    "    shuffle=False\n",
    ")\n",
    "test_dataloader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 3, 32, 32])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter = iter(train_dataloader)\n",
    "imgs, labels = train_iter.next()\n",
    "imgs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 7, 5, 5, 9, 6, 9, 9, 5, 4, 1, 6, 5, 8, 2, 9, 8, 3, 9, 9, 5, 2, 9, 9,\n",
       "        9, 1, 6, 9, 1, 1, 2, 2, 0, 1, 1, 4, 3, 3, 6, 7, 4, 2, 8, 2, 9, 6, 7, 8,\n",
       "        8, 2, 7, 9, 5, 0, 5, 3, 4, 0, 5, 3, 1, 1, 4, 2, 0, 2, 5, 4, 1, 6, 3, 7,\n",
       "        1, 5, 7, 4, 3, 2, 6, 3, 1, 3, 1, 1, 5, 9, 5, 5, 1, 4, 7, 0, 5, 0, 7, 5,\n",
       "        6, 8, 3, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels # 正解ラベルの確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ネットワークモデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(3*32 * 32, 600),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0,2),\n",
    "            nn.Linear(600, 600),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(600, 10),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        output = self.classifier(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "net = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 損失関数、最適化関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 2.2780460176467896\n",
      "epoch: 1, loss: 2.222033371448517\n",
      "epoch: 2, loss: 2.172431688785553\n",
      "epoch: 3, loss: 2.1164715378284455\n",
      "epoch: 4, loss: 2.0581098670959475\n",
      "epoch: 5, loss: 2.0217964968681335\n",
      "epoch: 6, loss: 1.928735027551651\n",
      "epoch: 7, loss: 1.849016846179962\n",
      "epoch: 8, loss: 1.816919284582138\n",
      "epoch: 9, loss: 1.7894398176670074\n",
      "epoch: 10, loss: 1.7669564740657806\n",
      "epoch: 11, loss: 1.7452970690727234\n",
      "epoch: 12, loss: 1.7245732789039612\n",
      "epoch: 13, loss: 1.7043749513626099\n",
      "epoch: 14, loss: 1.685697230577469\n",
      "epoch: 15, loss: 1.669877507686615\n",
      "epoch: 16, loss: 1.6553406014442444\n",
      "epoch: 17, loss: 1.6387480006217956\n",
      "epoch: 18, loss: 1.6211267189979552\n",
      "epoch: 19, loss: 1.6095947177410126\n",
      "epoch: 20, loss: 1.5981989705562591\n",
      "epoch: 21, loss: 1.5824703829288482\n",
      "epoch: 22, loss: 1.567874300956726\n",
      "epoch: 23, loss: 1.5582439868450164\n",
      "epoch: 24, loss: 1.5459020698070527\n",
      "epoch: 25, loss: 1.533275943517685\n",
      "epoch: 26, loss: 1.522409946203232\n",
      "epoch: 27, loss: 1.5131111004352569\n",
      "epoch: 28, loss: 1.5022057232856751\n",
      "epoch: 29, loss: 1.4919212839603424\n",
      "epoch: 30, loss: 1.4799476375579834\n",
      "epoch: 31, loss: 1.4722667558193208\n",
      "epoch: 32, loss: 1.462206746339798\n",
      "epoch: 33, loss: 1.4534377543926238\n",
      "epoch: 34, loss: 1.4427156004905701\n",
      "epoch: 35, loss: 1.4341166615486145\n",
      "epoch: 36, loss: 1.4271792850494385\n",
      "epoch: 37, loss: 1.416228021144867\n",
      "epoch: 38, loss: 1.4074293580055237\n",
      "epoch: 39, loss: 1.3971988062858582\n",
      "epoch: 40, loss: 1.3897851021289827\n",
      "epoch: 41, loss: 1.3813328373432159\n",
      "epoch: 42, loss: 1.3777106370925902\n",
      "epoch: 43, loss: 1.3656668701171875\n",
      "epoch: 44, loss: 1.3549573516845703\n",
      "epoch: 45, loss: 1.3484212927818298\n",
      "epoch: 46, loss: 1.342087992668152\n",
      "epoch: 47, loss: 1.336800088405609\n",
      "epoch: 48, loss: 1.3228007740974426\n",
      "epoch: 49, loss: 1.3195276279449464\n",
      "epoch: 50, loss: 1.3149599924087525\n",
      "epoch: 51, loss: 1.305622340798378\n",
      "epoch: 52, loss: 1.2983780119419097\n",
      "epoch: 53, loss: 1.2920202827453613\n",
      "epoch: 54, loss: 1.2831455594301224\n",
      "epoch: 55, loss: 1.270540065050125\n",
      "epoch: 56, loss: 1.2691614100933075\n",
      "epoch: 57, loss: 1.2670568512678146\n",
      "epoch: 58, loss: 1.2515358016490936\n",
      "epoch: 59, loss: 1.2504485938549041\n",
      "epoch: 60, loss: 1.24741614818573\n",
      "epoch: 61, loss: 1.2343906352519989\n",
      "epoch: 62, loss: 1.232973985671997\n",
      "epoch: 63, loss: 1.2191787676811219\n",
      "epoch: 64, loss: 1.2161122806072235\n",
      "epoch: 65, loss: 1.2099112664461136\n",
      "epoch: 66, loss: 1.2003583154678346\n",
      "epoch: 67, loss: 1.1951126774549485\n",
      "epoch: 68, loss: 1.1874296944141387\n",
      "epoch: 69, loss: 1.1818159482479096\n",
      "epoch: 70, loss: 1.1699928817749023\n",
      "epoch: 71, loss: 1.1689160865545274\n",
      "epoch: 72, loss: 1.1613475651741028\n",
      "epoch: 73, loss: 1.1544480402469635\n",
      "epoch: 74, loss: 1.150781169295311\n",
      "epoch: 75, loss: 1.140906125664711\n",
      "epoch: 76, loss: 1.133747318148613\n",
      "epoch: 77, loss: 1.124302066206932\n",
      "epoch: 78, loss: 1.1161598490476607\n",
      "epoch: 79, loss: 1.1182069509029389\n",
      "epoch: 80, loss: 1.1030509469509124\n",
      "epoch: 81, loss: 1.0982903558015824\n",
      "epoch: 82, loss: 1.0985650980472565\n",
      "epoch: 83, loss: 1.0910576790571214\n",
      "epoch: 84, loss: 1.0829779725074768\n",
      "epoch: 85, loss: 1.0739762456417083\n",
      "epoch: 86, loss: 1.0657700815200806\n",
      "epoch: 87, loss: 1.0633957679271697\n",
      "epoch: 88, loss: 1.0618331993818284\n",
      "epoch: 89, loss: 1.0470827881097793\n",
      "epoch: 90, loss: 1.0455684061050414\n",
      "epoch: 91, loss: 1.0449604499340057\n",
      "epoch: 92, loss: 1.04020574259758\n",
      "epoch: 93, loss: 1.0313382865190506\n",
      "epoch: 94, loss: 1.01709220290184\n",
      "epoch: 95, loss: 1.0165363063812256\n",
      "epoch: 96, loss: 1.0072594406604767\n",
      "epoch: 97, loss: 1.000886638402939\n",
      "epoch: 98, loss: 0.9887274839878082\n",
      "epoch: 99, loss: 1.006817822933197\n",
      "epoch: 100, loss: 0.9903059957027436\n",
      "epoch: 101, loss: 0.9801714638471604\n",
      "epoch: 102, loss: 0.9721544858217239\n",
      "epoch: 103, loss: 0.9697255525588989\n",
      "epoch: 104, loss: 0.9541967933177948\n",
      "epoch: 105, loss: 0.9545998392105103\n",
      "epoch: 106, loss: 0.9481868597269059\n",
      "epoch: 107, loss: 0.9458382865190506\n",
      "epoch: 108, loss: 0.9361232941150666\n",
      "epoch: 109, loss: 0.9302856080532074\n",
      "epoch: 110, loss: 0.9282835363149643\n",
      "epoch: 111, loss: 0.9213480043411255\n",
      "epoch: 112, loss: 0.9141622779369354\n",
      "epoch: 113, loss: 0.9094936293363571\n",
      "epoch: 114, loss: 0.902938750386238\n",
      "epoch: 115, loss: 0.9030952152013779\n",
      "epoch: 116, loss: 0.9023119925260544\n",
      "epoch: 117, loss: 0.8864387098550797\n",
      "epoch: 118, loss: 0.8812998660802841\n",
      "epoch: 119, loss: 0.8739447224140168\n",
      "epoch: 120, loss: 0.8716772263646125\n",
      "epoch: 121, loss: 0.8670002583861351\n",
      "epoch: 122, loss: 0.8545445224046707\n",
      "epoch: 123, loss: 0.8598475776910782\n",
      "epoch: 124, loss: 0.8621812001466751\n",
      "epoch: 125, loss: 0.84212710916996\n",
      "epoch: 126, loss: 0.8395759711265564\n",
      "epoch: 127, loss: 0.8330096464157104\n",
      "epoch: 128, loss: 0.8212620695829391\n",
      "epoch: 129, loss: 0.8216056044101715\n",
      "epoch: 130, loss: 0.815754928946495\n",
      "epoch: 131, loss: 0.8094180247187615\n",
      "epoch: 132, loss: 0.7962724915742874\n",
      "epoch: 133, loss: 0.8025059676766395\n",
      "epoch: 134, loss: 0.7969272639155388\n",
      "epoch: 135, loss: 0.7947799332737923\n",
      "epoch: 136, loss: 0.7821737640500068\n",
      "epoch: 137, loss: 0.780962885081768\n",
      "epoch: 138, loss: 0.7751727069020271\n",
      "epoch: 139, loss: 0.7679241164326668\n",
      "epoch: 140, loss: 0.7717663874626159\n",
      "epoch: 141, loss: 0.761754457950592\n",
      "epoch: 142, loss: 0.7609580993652344\n",
      "epoch: 143, loss: 0.7536781662106514\n",
      "epoch: 144, loss: 0.745853011071682\n",
      "epoch: 145, loss: 0.742661409497261\n",
      "epoch: 146, loss: 0.741652357339859\n",
      "epoch: 147, loss: 0.7305531945824623\n",
      "epoch: 148, loss: 0.7216580414772034\n",
      "epoch: 149, loss: 0.7238384684324265\n",
      "epoch: 150, loss: 0.7301435323357582\n",
      "epoch: 151, loss: 0.7192956843972206\n",
      "epoch: 152, loss: 0.7119768730401993\n",
      "epoch: 153, loss: 0.7033484249114991\n",
      "epoch: 154, loss: 0.6980424451828003\n",
      "epoch: 155, loss: 0.6952798026800155\n",
      "epoch: 156, loss: 0.6958389852643013\n",
      "epoch: 157, loss: 0.7015472937822342\n",
      "epoch: 158, loss: 0.6771498736143112\n",
      "epoch: 159, loss: 0.6750769746303559\n",
      "epoch: 160, loss: 0.6732817747592926\n",
      "epoch: 161, loss: 0.670178798019886\n",
      "epoch: 162, loss: 0.6643077968358994\n",
      "epoch: 163, loss: 0.6621085777282715\n",
      "epoch: 164, loss: 0.6582447572946548\n",
      "epoch: 165, loss: 0.6598126019239425\n",
      "epoch: 166, loss: 0.6470903690457344\n",
      "epoch: 167, loss: 0.6497294182181358\n",
      "epoch: 168, loss: 0.6452832180261612\n",
      "epoch: 169, loss: 0.6365208398103714\n",
      "epoch: 170, loss: 0.6429962239861489\n",
      "epoch: 171, loss: 0.6411480703353882\n",
      "epoch: 172, loss: 0.633181116104126\n",
      "epoch: 173, loss: 0.6177350499629974\n",
      "epoch: 174, loss: 0.6238897827863693\n",
      "epoch: 175, loss: 0.6128547201752663\n",
      "epoch: 176, loss: 0.6327633293867111\n",
      "epoch: 177, loss: 0.6214850801229477\n",
      "epoch: 178, loss: 0.6259256882667541\n",
      "epoch: 179, loss: 0.6047586537003518\n",
      "epoch: 180, loss: 0.5958678357005119\n",
      "epoch: 181, loss: 0.5965663610696793\n",
      "epoch: 182, loss: 0.6057289107441902\n",
      "epoch: 183, loss: 0.5895919688940048\n",
      "epoch: 184, loss: 0.5921170702576637\n",
      "epoch: 186, loss: 0.5825458715558052\n",
      "epoch: 187, loss: 0.590830977499485\n",
      "epoch: 188, loss: 0.5750966694951057\n",
      "epoch: 189, loss: 0.5751562215089798\n",
      "epoch: 190, loss: 0.5656530607938767\n",
      "epoch: 191, loss: 0.554647611439228\n",
      "epoch: 192, loss: 0.5580129989981651\n",
      "epoch: 193, loss: 0.5712043750286102\n",
      "epoch: 194, loss: 0.5532660037279129\n",
      "epoch: 195, loss: 0.5586514677405358\n",
      "epoch: 196, loss: 0.5499759269952774\n",
      "epoch: 197, loss: 0.5582277705669403\n",
      "epoch: 198, loss: 0.5423171672224999\n",
      "epoch: 199, loss: 0.5375210303664207\n",
      "epoch: 200, loss: 0.5366482563614845\n",
      "epoch: 201, loss: 0.5416872435808182\n",
      "epoch: 202, loss: 0.5250045621991157\n",
      "epoch: 203, loss: 0.5390616561174393\n",
      "epoch: 204, loss: 0.5252792407870293\n",
      "epoch: 205, loss: 0.5245315366983414\n",
      "epoch: 206, loss: 0.5253355318307876\n",
      "epoch: 207, loss: 0.5122059994935989\n",
      "epoch: 208, loss: 0.511393171608448\n",
      "epoch: 209, loss: 0.5128660784959793\n",
      "epoch: 210, loss: 0.5060316253304481\n",
      "epoch: 211, loss: 0.5077831163704395\n",
      "epoch: 212, loss: 0.5156239008307457\n",
      "epoch: 213, loss: 0.4884576733708382\n",
      "epoch: 214, loss: 0.4949960639476776\n",
      "epoch: 215, loss: 0.4963803882598877\n",
      "epoch: 216, loss: 0.48881624680757524\n",
      "epoch: 217, loss: 0.5016015929579735\n",
      "epoch: 218, loss: 0.48766212928295133\n",
      "epoch: 219, loss: 0.4912526204288006\n",
      "epoch: 220, loss: 0.4902897543311119\n",
      "epoch: 221, loss: 0.48421832832694056\n",
      "epoch: 222, loss: 0.4705370478928089\n",
      "epoch: 223, loss: 0.4843029657304287\n",
      "epoch: 224, loss: 0.4754684962034225\n",
      "epoch: 225, loss: 0.4795891179442406\n",
      "epoch: 226, loss: 0.4700066852867603\n",
      "epoch: 227, loss: 0.4766339432299137\n",
      "epoch: 228, loss: 0.4766538487970829\n",
      "epoch: 229, loss: 0.4574064199626446\n",
      "epoch: 230, loss: 0.45908959046006204\n",
      "epoch: 231, loss: 0.4648012772798538\n",
      "epoch: 232, loss: 0.4461294283270836\n",
      "epoch: 233, loss: 0.4490056301653385\n",
      "epoch: 234, loss: 0.4764910626411438\n",
      "epoch: 235, loss: 0.4676330316066742\n",
      "epoch: 236, loss: 0.4489277815818787\n",
      "epoch: 237, loss: 0.45587362241744994\n",
      "epoch: 238, loss: 0.4498243724703789\n",
      "epoch: 239, loss: 0.44414564964175224\n",
      "epoch: 240, loss: 0.44535455188155176\n",
      "epoch: 241, loss: 0.43179631942510605\n",
      "epoch: 242, loss: 0.42856254169344904\n",
      "epoch: 243, loss: 0.45415154898166654\n",
      "epoch: 244, loss: 0.4359045615196228\n",
      "epoch: 245, loss: 0.4482323809564114\n",
      "epoch: 246, loss: 0.42976906841993334\n",
      "epoch: 247, loss: 0.43042694792151454\n",
      "epoch: 248, loss: 0.4350034509003162\n",
      "epoch: 249, loss: 0.43346886610984803\n",
      "epoch: 250, loss: 0.4248175483644009\n",
      "epoch: 251, loss: 0.41459598127007485\n",
      "epoch: 252, loss: 0.4221632095873356\n",
      "epoch: 253, loss: 0.41591784474253657\n",
      "epoch: 254, loss: 0.4220623522698879\n",
      "epoch: 255, loss: 0.43952102640271185\n",
      "epoch: 256, loss: 0.42659623646736144\n",
      "epoch: 257, loss: 0.411770420819521\n",
      "epoch: 258, loss: 0.4125873847305775\n",
      "epoch: 259, loss: 0.41388698264956475\n",
      "epoch: 260, loss: 0.419909972935915\n",
      "epoch: 261, loss: 0.40598371651768683\n",
      "epoch: 262, loss: 0.40644783359766007\n",
      "epoch: 263, loss: 0.4036661175787449\n",
      "epoch: 264, loss: 0.40479436710476874\n",
      "epoch: 265, loss: 0.4142547704577446\n",
      "epoch: 266, loss: 0.4092700027525425\n",
      "epoch: 267, loss: 0.41518327447772024\n",
      "epoch: 268, loss: 0.4076452962756157\n",
      "epoch: 269, loss: 0.40042030295729636\n",
      "epoch: 270, loss: 0.394458543330431\n",
      "epoch: 271, loss: 0.411003069370985\n",
      "epoch: 272, loss: 0.3956177500784397\n",
      "epoch: 273, loss: 0.41249048697948454\n",
      "epoch: 274, loss: 0.3916438817977905\n",
      "epoch: 275, loss: 0.39095274284482\n",
      "epoch: 276, loss: 0.39405637642741204\n",
      "epoch: 277, loss: 0.4140578728020191\n",
      "epoch: 278, loss: 0.40166099825501445\n",
      "epoch: 279, loss: 0.38586323472857476\n",
      "epoch: 280, loss: 0.39444416788220404\n",
      "epoch: 281, loss: 0.3792787444591522\n",
      "epoch: 282, loss: 0.43179915156960486\n",
      "epoch: 283, loss: 0.38829780930280683\n",
      "epoch: 284, loss: 0.38889045763015745\n",
      "epoch: 285, loss: 0.3730692449212074\n",
      "epoch: 286, loss: 0.3790592395365238\n",
      "epoch: 287, loss: 0.37937756438553333\n",
      "epoch: 288, loss: 0.37507237812876704\n",
      "epoch: 289, loss: 0.37498732733726503\n",
      "epoch: 290, loss: 0.38578667655587195\n",
      "epoch: 291, loss: 0.37545517951250074\n",
      "epoch: 292, loss: 0.39615939527750016\n",
      "epoch: 293, loss: 0.3634317398071289\n",
      "epoch: 294, loss: 0.3695219715833664\n",
      "epoch: 295, loss: 0.3767129213511944\n",
      "epoch: 296, loss: 0.3839206397533417\n",
      "epoch: 297, loss: 0.38814937031269076\n",
      "epoch: 298, loss: 0.3908676028251648\n",
      "epoch: 299, loss: 0.4040327571928501\n",
      "epoch: 300, loss: 0.3818413526415825\n",
      "epoch: 301, loss: 0.36594629484415053\n",
      "epoch: 302, loss: 0.36407246938347815\n",
      "epoch: 303, loss: 0.36441281439363954\n",
      "epoch: 304, loss: 0.3670155645608902\n",
      "epoch: 305, loss: 0.3730944403409958\n",
      "epoch: 306, loss: 0.3615286286175251\n",
      "epoch: 307, loss: 0.3744352551102638\n",
      "epoch: 308, loss: 0.3680996482223272\n",
      "epoch: 309, loss: 0.3556338851451874\n",
      "epoch: 310, loss: 0.3614900204241276\n",
      "epoch: 311, loss: 0.3609312853217125\n",
      "epoch: 312, loss: 0.350347668081522\n",
      "epoch: 313, loss: 0.35927234379947187\n",
      "epoch: 314, loss: 0.35336255198717115\n",
      "epoch: 315, loss: 0.3949470402300358\n",
      "epoch: 316, loss: 0.36188013020157817\n",
      "epoch: 317, loss: 0.36764129462838174\n",
      "epoch: 318, loss: 0.35355550214648246\n",
      "epoch: 319, loss: 0.357877565652132\n",
      "epoch: 320, loss: 0.3463490084409714\n",
      "epoch: 321, loss: 0.3569818473756313\n",
      "epoch: 322, loss: 0.3458733496367931\n",
      "epoch: 323, loss: 0.3450396904349327\n",
      "epoch: 324, loss: 0.35222229158878327\n",
      "epoch: 325, loss: 0.3457593147754669\n",
      "epoch: 326, loss: 0.35162540408968923\n",
      "epoch: 327, loss: 0.36129502761363985\n",
      "epoch: 328, loss: 0.3466011000573635\n",
      "epoch: 329, loss: 0.34743289004266265\n",
      "epoch: 330, loss: 0.34925161439180374\n",
      "epoch: 331, loss: 0.3539441812932491\n",
      "epoch: 332, loss: 0.34389027988910675\n",
      "epoch: 333, loss: 0.3446650030016899\n",
      "epoch: 334, loss: 0.34061992561817167\n",
      "epoch: 335, loss: 0.34024849614501\n",
      "epoch: 336, loss: 0.33656764595210553\n",
      "epoch: 337, loss: 0.3400619574487209\n",
      "epoch: 338, loss: 0.35572952070832253\n",
      "epoch: 339, loss: 0.34245077869296076\n",
      "epoch: 340, loss: 0.3400746907293797\n",
      "epoch: 341, loss: 0.34086894086003305\n",
      "epoch: 342, loss: 0.33512686608731745\n",
      "epoch: 343, loss: 0.35011628666520117\n",
      "epoch: 344, loss: 0.3577874260991812\n",
      "epoch: 345, loss: 0.3418086367547512\n",
      "epoch: 346, loss: 0.3366691316664219\n",
      "epoch: 347, loss: 0.33695188933610914\n",
      "epoch: 348, loss: 0.3440735580921173\n",
      "epoch: 349, loss: 0.3405226466357708\n",
      "epoch: 350, loss: 0.33498044964671136\n",
      "epoch: 351, loss: 0.34045988646149633\n",
      "epoch: 352, loss: 0.34055151069164274\n",
      "epoch: 353, loss: 0.35037651148438453\n",
      "epoch: 354, loss: 0.3391224333941936\n",
      "epoch: 355, loss: 0.3346776864528656\n",
      "epoch: 356, loss: 0.33473842826485634\n",
      "epoch: 357, loss: 0.33560374614596367\n",
      "epoch: 358, loss: 0.34609374099969864\n",
      "epoch: 359, loss: 0.34158000522851945\n",
      "epoch: 360, loss: 0.37930268412828444\n",
      "epoch: 361, loss: 0.33544306866824625\n",
      "epoch: 362, loss: 0.3309903038293123\n",
      "epoch: 363, loss: 0.33181793746352195\n",
      "epoch: 364, loss: 0.34150495813786985\n",
      "epoch: 365, loss: 0.3328424998521805\n",
      "epoch: 366, loss: 0.33925535836815834\n",
      "epoch: 367, loss: 0.33297420185804366\n",
      "epoch: 368, loss: 0.33702763202786445\n",
      "epoch: 369, loss: 0.3334330765902996\n",
      "epoch: 370, loss: 0.33488041639328003\n",
      "epoch: 371, loss: 0.32927692660689356\n",
      "epoch: 372, loss: 0.32864969193935395\n",
      "epoch: 373, loss: 0.3462735574245453\n",
      "epoch: 374, loss: 0.3370202530771494\n",
      "epoch: 375, loss: 0.3294985986053944\n",
      "epoch: 376, loss: 0.32874282978475095\n",
      "epoch: 377, loss: 0.33019434425234795\n",
      "epoch: 378, loss: 0.3275746901631355\n",
      "epoch: 379, loss: 0.3703652568757534\n",
      "epoch: 380, loss: 0.53958412155509\n",
      "epoch: 381, loss: 0.38796241202950477\n",
      "epoch: 382, loss: 0.3340188825875521\n",
      "epoch: 383, loss: 0.3355948807001114\n",
      "epoch: 384, loss: 0.33589165300130847\n",
      "epoch: 385, loss: 0.3365075522661209\n",
      "epoch: 386, loss: 0.3293829496055841\n",
      "epoch: 387, loss: 0.3266055419743061\n",
      "epoch: 388, loss: 0.3266143663078547\n",
      "epoch: 389, loss: 0.3284757003486157\n",
      "epoch: 390, loss: 0.33179765932261945\n",
      "epoch: 391, loss: 0.32458206701278686\n",
      "epoch: 392, loss: 0.3267359038591385\n",
      "epoch: 393, loss: 0.3291493787765503\n",
      "epoch: 394, loss: 0.32845056299865244\n",
      "epoch: 395, loss: 0.4582665605843067\n",
      "epoch: 396, loss: 0.5019566501379014\n",
      "epoch: 397, loss: 0.4312343896180391\n",
      "epoch: 398, loss: 0.34818217396736145\n",
      "epoch: 399, loss: 0.34270590847730636\n",
      "epoch: 400, loss: 0.3377478185296059\n",
      "epoch: 401, loss: 0.3280866400003433\n",
      "epoch: 402, loss: 0.3325967895835638\n",
      "epoch: 403, loss: 0.32922228607535364\n",
      "epoch: 404, loss: 0.33192780634760854\n",
      "epoch: 405, loss: 0.3277919054329395\n",
      "epoch: 406, loss: 0.33017418552935124\n",
      "epoch: 407, loss: 0.3293278561234474\n",
      "epoch: 408, loss: 0.327594079554081\n",
      "epoch: 409, loss: 0.33015139468014243\n",
      "epoch: 410, loss: 0.3249324656426907\n",
      "epoch: 411, loss: 0.3248996584266424\n",
      "epoch: 412, loss: 0.32393403051793573\n",
      "epoch: 413, loss: 0.3251571305096149\n",
      "epoch: 414, loss: 0.32683988428115845\n",
      "epoch: 415, loss: 0.324149829223752\n",
      "epoch: 416, loss: 0.3242934365570545\n",
      "epoch: 417, loss: 0.3246558865308762\n",
      "epoch: 418, loss: 0.3267537051141262\n",
      "epoch: 419, loss: 0.3312142053246498\n",
      "epoch: 420, loss: 0.32802426628768444\n",
      "epoch: 421, loss: 0.3247076361477375\n",
      "epoch: 422, loss: 0.3234214721620083\n",
      "epoch: 423, loss: 0.3244898601770401\n",
      "epoch: 424, loss: 0.3240132775902748\n",
      "epoch: 425, loss: 0.32256790390610696\n",
      "epoch: 426, loss: 0.3231746006309986\n",
      "epoch: 427, loss: 0.3230113163292408\n",
      "epoch: 428, loss: 0.3224302456229925\n",
      "epoch: 429, loss: 0.32282772697508333\n",
      "epoch: 430, loss: 0.32209686854481695\n",
      "epoch: 431, loss: 0.3233725773990154\n",
      "epoch: 432, loss: 0.32363950584828854\n",
      "epoch: 433, loss: 0.33008395424485204\n",
      "epoch: 434, loss: 0.3249142464399338\n",
      "epoch: 435, loss: 0.32168078937381506\n",
      "epoch: 436, loss: 0.32570331180095674\n",
      "epoch: 437, loss: 0.3248101400732994\n",
      "epoch: 438, loss: 0.3237852684855461\n",
      "epoch: 439, loss: 0.3240314608216286\n",
      "epoch: 440, loss: 0.32226157853007315\n",
      "epoch: 441, loss: 0.3240728341341019\n",
      "epoch: 442, loss: 0.32426008230447767\n",
      "epoch: 443, loss: 0.3228989309966564\n",
      "epoch: 444, loss: 0.3289549688696861\n",
      "epoch: 445, loss: 0.3220450437068939\n",
      "epoch: 446, loss: 0.3236152827292681\n",
      "epoch: 447, loss: 0.32815342622995375\n",
      "epoch: 448, loss: 0.3228772774040699\n",
      "epoch: 449, loss: 0.3222517118304968\n",
      "epoch: 450, loss: 0.32915645426511764\n",
      "epoch: 451, loss: 0.3244286565035582\n",
      "epoch: 452, loss: 0.32336082062125204\n",
      "epoch: 453, loss: 0.3259341605603695\n",
      "epoch: 454, loss: 0.32284210942685604\n",
      "epoch: 455, loss: 0.3251389393210411\n",
      "epoch: 456, loss: 0.34032247044146063\n",
      "epoch: 457, loss: 0.32478633223474024\n",
      "epoch: 458, loss: 0.3245450347363949\n",
      "epoch: 459, loss: 0.3227141835987568\n",
      "epoch: 460, loss: 0.324484651774168\n",
      "epoch: 461, loss: 0.3218437665402889\n",
      "epoch: 462, loss: 0.330524604678154\n",
      "epoch: 463, loss: 0.3248402301967144\n",
      "epoch: 464, loss: 0.3232153313308954\n",
      "epoch: 465, loss: 0.3220684544146061\n",
      "epoch: 466, loss: 0.32237146684527396\n",
      "epoch: 467, loss: 0.3281692249625921\n",
      "epoch: 468, loss: 0.3233283503949642\n",
      "epoch: 469, loss: 0.32359217281639574\n",
      "epoch: 470, loss: 0.3238509419262409\n",
      "epoch: 471, loss: 0.32337920264899733\n",
      "epoch: 472, loss: 0.4238878900259733\n",
      "epoch: 473, loss: 0.6856031010746956\n",
      "epoch: 474, loss: 0.5672297428846359\n",
      "epoch: 475, loss: 0.5350922736227512\n",
      "epoch: 476, loss: 0.5466410134136677\n",
      "epoch: 477, loss: 0.42310017535090444\n",
      "epoch: 478, loss: 0.3528349206149578\n",
      "epoch: 479, loss: 0.33884867249429224\n",
      "epoch: 480, loss: 0.36301274168491365\n",
      "epoch: 481, loss: 0.3504001315534115\n",
      "epoch: 482, loss: 0.3316663685441017\n",
      "epoch: 483, loss: 0.3300270488858223\n",
      "epoch: 484, loss: 0.3320579864233732\n",
      "epoch: 485, loss: 0.3306580512076616\n",
      "epoch: 486, loss: 0.3252851950675249\n",
      "epoch: 487, loss: 0.325287899941206\n",
      "epoch: 488, loss: 0.32951981022953986\n",
      "epoch: 489, loss: 0.32234136497974397\n",
      "epoch: 490, loss: 0.32776270680129527\n",
      "epoch: 491, loss: 0.3237694378495216\n",
      "epoch: 492, loss: 0.3220101802945137\n",
      "epoch: 493, loss: 0.32582742044329643\n",
      "epoch: 494, loss: 0.32310217928886414\n",
      "epoch: 495, loss: 0.32310233552753925\n",
      "epoch: 496, loss: 0.3229687591046095\n",
      "epoch: 497, loss: 0.3218630649447441\n",
      "epoch: 498, loss: 0.32365619027614595\n",
      "epoch: 499, loss: 0.32526791363954544\n",
      "CPU times: user 43min 55s, sys: 10.8 s, total: 44min 6s\n",
      "Wall time: 42min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_epochs = 400\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    net.train()\n",
    "\n",
    "    for imgs, labels in train_dataloader:\n",
    "        imgs = imgs.view(num_batches, -1)\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(imgs)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        running_loss += loss.item()\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    running_loss /= len(train_dataloader)\n",
    "    losses.append(running_loss)\n",
    "\n",
    "    print(\"epoch: {}, loss: {}\".format(epoch, running_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f19b01b2668>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses) # 誤差が減少していることを確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テスト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正解ラベル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 5, 2, 6, 3, 6, 5, 1, 2, 5, 3, 3, 9, 1, 3, 3, 7, 2, 3, 5, 9, 6, 8, 9,\n",
       "        6, 1, 3, 2, 3, 7, 4, 0, 9, 0, 8, 6, 0, 4, 2, 7, 9, 4, 7, 9, 8, 2, 9, 3,\n",
       "        5, 7, 3, 4, 3, 3, 6, 3, 3, 5, 1, 1, 3, 3, 1, 3, 2, 6, 0, 5, 4, 0, 2, 1,\n",
       "        9, 9, 8, 1, 3, 8, 4, 8, 9, 4, 3, 8, 9, 8, 8, 2, 8, 1, 8, 0, 0, 4, 6, 0,\n",
       "        2, 8, 8, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter = iter(train_dataloader)\n",
    "imgs, labels = train_iter.next()\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 予測結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 7, 2, 6, 3, 6, 7, 1, 2, 7, 3, 3, 9, 1, 3, 3, 7, 2, 3, 7, 9, 6, 8, 9,\n",
       "        6, 1, 3, 2, 3, 7, 4, 0, 9, 0, 7, 6, 0, 4, 2, 7, 9, 4, 7, 9, 8, 2, 9, 3,\n",
       "        7, 7, 3, 7, 3, 2, 6, 3, 3, 7, 1, 1, 3, 3, 1, 3, 2, 7, 0, 7, 4, 0, 2, 1,\n",
       "        9, 9, 8, 1, 3, 8, 4, 8, 9, 4, 3, 8, 9, 8, 8, 2, 8, 1, 8, 0, 0, 4, 6, 0,\n",
       "        2, 8, 8, 0], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_gpu = imgs.view(100, -1).to(device)\n",
    "output= model(imgs_gpu)\n",
    "pred = torch.argmax(output, dim=1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "\n",
    "for imgs, labels in test_dataloader:\n",
    "    labels = labels.to(device)\n",
    "    imgs_gpu = imgs.view(num_batches, -1).to(device)\n",
    "    output = model(imgs_gpu)\n",
    "    pred = torch.argmax(output, dim=1)\n",
    "    s += sum(labels==pred)\n",
    "    \n",
    "int(s)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
